{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib64/python3.9/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib64/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3.9/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/cloud/.local/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2000, 21)\n",
      "\n",
      "First 5 rows:\n",
      "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
      "0            842     0          2.2         0   1       0           7    0.6   \n",
      "1           1021     1          0.5         1   0       1          53    0.7   \n",
      "2            563     1          0.5         1   2       1          41    0.9   \n",
      "3            615     1          2.5         0   0       0          10    0.8   \n",
      "4           1821     1          1.2         0  13       1          44    0.6   \n",
      "\n",
      "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
      "0        188        2  ...         20       756  2549     9     7         19   \n",
      "1        136        3  ...        905      1988  2631    17     3          7   \n",
      "2        145        5  ...       1263      1716  2603    11     2          9   \n",
      "3        131        6  ...       1216      1786  2769    16     8         11   \n",
      "4        141        2  ...       1208      1212  1411     8     2         15   \n",
      "\n",
      "   three_g  touch_screen  wifi  price_range  \n",
      "0        0             0     1            1  \n",
      "1        1             1     0            2  \n",
      "2        1             1     0            2  \n",
      "3        1             0     0            2  \n",
      "4        1             1     0            1  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Target distribution (price_range):\n",
      "price_range\n",
      "1    500\n",
      "2    500\n",
      "3    500\n",
      "0    500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('mobile_price.csv')\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nTarget distribution (price_range):\")\n",
    "print(df['price_range'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Info ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   battery_power  2000 non-null   int64  \n",
      " 1   blue           2000 non-null   int64  \n",
      " 2   clock_speed    2000 non-null   float64\n",
      " 3   dual_sim       2000 non-null   int64  \n",
      " 4   fc             2000 non-null   int64  \n",
      " 5   four_g         2000 non-null   int64  \n",
      " 6   int_memory     2000 non-null   int64  \n",
      " 7   m_dep          2000 non-null   float64\n",
      " 8   mobile_wt      2000 non-null   int64  \n",
      " 9   n_cores        2000 non-null   int64  \n",
      " 10  pc             2000 non-null   int64  \n",
      " 11  px_height      2000 non-null   int64  \n",
      " 12  px_width       2000 non-null   int64  \n",
      " 13  ram            2000 non-null   int64  \n",
      " 14  sc_h           2000 non-null   int64  \n",
      " 15  sc_w           2000 non-null   int64  \n",
      " 16  talk_time      2000 non-null   int64  \n",
      " 17  three_g        2000 non-null   int64  \n",
      " 18  touch_screen   2000 non-null   int64  \n",
      " 19  wifi           2000 non-null   int64  \n",
      " 20  price_range    2000 non-null   int64  \n",
      "dtypes: float64(2), int64(19)\n",
      "memory usage: 328.2 KB\n",
      "None\n",
      "\n",
      "=== Missing Values ===\n",
      "battery_power    0\n",
      "blue             0\n",
      "clock_speed      0\n",
      "dual_sim         0\n",
      "fc               0\n",
      "four_g           0\n",
      "int_memory       0\n",
      "m_dep            0\n",
      "mobile_wt        0\n",
      "n_cores          0\n",
      "pc               0\n",
      "px_height        0\n",
      "px_width         0\n",
      "ram              0\n",
      "sc_h             0\n",
      "sc_w             0\n",
      "talk_time        0\n",
      "three_g          0\n",
      "touch_screen     0\n",
      "wifi             0\n",
      "price_range      0\n",
      "dtype: int64\n",
      "\n",
      "=== Data Types ===\n",
      "battery_power      int64\n",
      "blue               int64\n",
      "clock_speed      float64\n",
      "dual_sim           int64\n",
      "fc                 int64\n",
      "four_g             int64\n",
      "int_memory         int64\n",
      "m_dep            float64\n",
      "mobile_wt          int64\n",
      "n_cores            int64\n",
      "pc                 int64\n",
      "px_height          int64\n",
      "px_width           int64\n",
      "ram                int64\n",
      "sc_h               int64\n",
      "sc_w               int64\n",
      "talk_time          int64\n",
      "three_g            int64\n",
      "touch_screen       int64\n",
      "wifi               int64\n",
      "price_range        int64\n",
      "dtype: object\n",
      "\n",
      "=== Target Distribution (price_range) ===\n",
      "price_range\n",
      "1    500\n",
      "2    500\n",
      "3    500\n",
      "0    500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage distribution:\n",
      "price_range\n",
      "1    25.0\n",
      "2    25.0\n",
      "3    25.0\n",
      "0    25.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== Features shape ===\n",
      "X shape: (2000, 20)\n",
      "y shape: (2000,)\n",
      "\n",
      "=== Train-Test Split ===\n",
      "Training set: (1600, 20)\n",
      "Testing set: (400, 20)\n",
      "Train target distribution:\n",
      "price_range\n",
      "1    400\n",
      "0    400\n",
      "3    400\n",
      "2    400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test target distribution:\n",
      "price_range\n",
      "3    100\n",
      "1    100\n",
      "0    100\n",
      "2    100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Check basic info\n",
    "print(\"=== Dataset Info ===\")\n",
    "print(df.info())\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 2. Check data types\n",
    "print(\"\\n=== Data Types ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 3. Check target distribution\n",
    "print(\"\\n=== Target Distribution (price_range) ===\")\n",
    "print(df['price_range'].value_counts())\n",
    "print(\"\\nPercentage distribution:\")\n",
    "print(df['price_range'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# 4. Separate features and target\n",
    "X = df.drop('price_range', axis=1)\n",
    "y = df['price_range']\n",
    "\n",
    "print(\"\\n=== Features shape ===\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# 5. Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n=== Train-Test Split ===\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")\n",
    "print(f\"Train target distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nTest target distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression Results ===\n",
      "Accuracy: 0.6700\n",
      "AUC Score: 0.8968\n",
      "Precision: 0.6809\n",
      "Recall: 0.6700\n",
      "F1 Score: 0.6746\n",
      "MCC Score: 0.5605\n",
      "\n",
      "Results stored for comparison table.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Initialize and train model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# 2. Make predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "y_pred_proba = log_reg.predict_proba(X_test)\n",
    "\n",
    "# 3. Calculate metrics\n",
    "# For multi-class AUC, we need to binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_test)\n",
    "y_test_bin = lb.transform(y_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test_bin, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# 4. Print results\n",
    "print(\"=== Logistic Regression Results ===\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"MCC Score: {mcc:.4f}\")\n",
    "\n",
    "# 5. Store results for later comparison\n",
    "log_reg_results = {\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Accuracy': accuracy,\n",
    "    'AUC': auc,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1': f1,\n",
    "    'MCC': mcc\n",
    "}\n",
    "\n",
    "# Create a results DataFrame to store all model results\n",
    "results_df = pd.DataFrame([log_reg_results])\n",
    "print(\"\\nResults stored for comparison table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Decision Tree Classifier Results ===\n",
      "Accuracy: 0.8300\n",
      "AUC Score: 0.8867\n",
      "Precision: 0.8319\n",
      "Recall: 0.8300\n",
      "F1 Score: 0.8302\n",
      "MCC Score: 0.7738\n",
      "\n",
      "Decision Tree results added to comparison table.\n",
      "\n",
      "Current Results Table:\n",
      "                 Model  Accuracy       AUC  Precision  Recall        F1  \\\n",
      "0  Logistic Regression      0.67  0.896800   0.680872    0.67  0.674565   \n",
      "1        Decision Tree      0.83  0.886667   0.831883    0.83  0.830168   \n",
      "\n",
      "        MCC  \n",
      "0  0.560537  \n",
      "1  0.773811  \n"
     ]
    }
   ],
   "source": [
    "# Import Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 1. Initialize and train Decision Tree\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# 2. Make predictions\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "y_pred_proba_dt = dt_clf.predict_proba(X_test)\n",
    "\n",
    "# 3. Calculate metrics\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "auc_dt = roc_auc_score(y_test_bin, y_pred_proba_dt, multi_class='ovr', average='weighted')\n",
    "precision_dt = precision_score(y_test, y_pred_dt, average='weighted')\n",
    "recall_dt = recall_score(y_test, y_pred_dt, average='weighted')\n",
    "f1_dt = f1_score(y_test, y_pred_dt, average='weighted')\n",
    "mcc_dt = matthews_corrcoef(y_test, y_pred_dt)\n",
    "\n",
    "# 4. Print results\n",
    "print(\"=== Decision Tree Classifier Results ===\")\n",
    "print(f\"Accuracy: {accuracy_dt:.4f}\")\n",
    "print(f\"AUC Score: {auc_dt:.4f}\")\n",
    "print(f\"Precision: {precision_dt:.4f}\")\n",
    "print(f\"Recall: {recall_dt:.4f}\")\n",
    "print(f\"F1 Score: {f1_dt:.4f}\")\n",
    "print(f\"MCC Score: {mcc_dt:.4f}\")\n",
    "\n",
    "# 5. Store results\n",
    "dt_results = {\n",
    "    'Model': 'Decision Tree',\n",
    "    'Accuracy': accuracy_dt,\n",
    "    'AUC': auc_dt,\n",
    "    'Precision': precision_dt,\n",
    "    'Recall': recall_dt,\n",
    "    'F1': f1_dt,\n",
    "    'MCC': mcc_dt\n",
    "}\n",
    "\n",
    "# Add to results DataFrame\n",
    "results_df = pd.concat([results_df, pd.DataFrame([dt_results])], ignore_index=True)\n",
    "print(\"\\nDecision Tree results added to comparison table.\")\n",
    "print(\"\\nCurrent Results Table:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== K-Nearest Neighbors Classifier Results ===\n",
      "Accuracy: 0.5000\n",
      "AUC Score: 0.7697\n",
      "Precision: 0.5211\n",
      "Recall: 0.5000\n",
      "F1 Score: 0.5054\n",
      "MCC Score: 0.3350\n",
      "\n",
      "KNN results added to comparison table.\n",
      "\n",
      "Current Results Table:\n",
      "                 Model  Accuracy       AUC  Precision  Recall        F1  \\\n",
      "0  Logistic Regression      0.67  0.896800   0.680872    0.67  0.674565   \n",
      "1        Decision Tree      0.83  0.886667   0.831883    0.83  0.830168   \n",
      "2  K-Nearest Neighbors      0.50  0.769750   0.521130    0.50  0.505355   \n",
      "\n",
      "        MCC  \n",
      "0  0.560537  \n",
      "1  0.773811  \n",
      "2  0.334993  \n"
     ]
    }
   ],
   "source": [
    "# Import KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Scale the features (important for KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Initialize and train KNN (using k=5 as default)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3. Make predictions\n",
    "y_pred_knn = knn_clf.predict(X_test_scaled)\n",
    "y_pred_proba_knn = knn_clf.predict_proba(X_test_scaled)\n",
    "\n",
    "# 4. Calculate metrics\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "auc_knn = roc_auc_score(y_test_bin, y_pred_proba_knn, multi_class='ovr', average='weighted')\n",
    "precision_knn = precision_score(y_test, y_pred_knn, average='weighted')\n",
    "recall_knn = recall_score(y_test, y_pred_knn, average='weighted')\n",
    "f1_knn = f1_score(y_test, y_pred_knn, average='weighted')\n",
    "mcc_knn = matthews_corrcoef(y_test, y_pred_knn)\n",
    "\n",
    "# 5. Print results\n",
    "print(\"=== K-Nearest Neighbors Classifier Results ===\")\n",
    "print(f\"Accuracy: {accuracy_knn:.4f}\")\n",
    "print(f\"AUC Score: {auc_knn:.4f}\")\n",
    "print(f\"Precision: {precision_knn:.4f}\")\n",
    "print(f\"Recall: {recall_knn:.4f}\")\n",
    "print(f\"F1 Score: {f1_knn:.4f}\")\n",
    "print(f\"MCC Score: {mcc_knn:.4f}\")\n",
    "\n",
    "# 6. Store results\n",
    "knn_results = {\n",
    "    'Model': 'K-Nearest Neighbors',\n",
    "    'Accuracy': accuracy_knn,\n",
    "    'AUC': auc_knn,\n",
    "    'Precision': precision_knn,\n",
    "    'Recall': recall_knn,\n",
    "    'F1': f1_knn,\n",
    "    'MCC': mcc_knn\n",
    "}\n",
    "\n",
    "# Add to results DataFrame\n",
    "results_df = pd.concat([results_df, pd.DataFrame([knn_results])], ignore_index=True)\n",
    "print(\"\\nKNN results added to comparison table.\")\n",
    "print(\"\\nCurrent Results Table:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Gaussian Naive Bayes Classifier Results ===\n",
      "Accuracy: 0.8100\n",
      "AUC Score: 0.9506\n",
      "Precision: 0.8113\n",
      "Recall: 0.8100\n",
      "F1 Score: 0.8105\n",
      "MCC Score: 0.7468\n",
      "\n",
      "Naive Bayes results added to comparison table.\n",
      "\n",
      "Current Results Table:\n",
      "                 Model  Accuracy       AUC  Precision  Recall        F1  \\\n",
      "0  Logistic Regression      0.67  0.896800   0.680872    0.67  0.674565   \n",
      "1        Decision Tree      0.83  0.886667   0.831883    0.83  0.830168   \n",
      "2  K-Nearest Neighbors      0.50  0.769750   0.521130    0.50  0.505355   \n",
      "3          Naive Bayes      0.81  0.950567   0.811326    0.81  0.810458   \n",
      "\n",
      "        MCC  \n",
      "0  0.560537  \n",
      "1  0.773811  \n",
      "2  0.334993  \n",
      "3  0.746804  \n"
     ]
    }
   ],
   "source": [
    "# Import Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 1. Initialize and train Gaussian Naive Bayes\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "\n",
    "# 2. Make predictions\n",
    "y_pred_nb = nb_clf.predict(X_test)\n",
    "y_pred_proba_nb = nb_clf.predict_proba(X_test)\n",
    "\n",
    "# 3. Calculate metrics\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "auc_nb = roc_auc_score(y_test_bin, y_pred_proba_nb, multi_class='ovr', average='weighted')\n",
    "precision_nb = precision_score(y_test, y_pred_nb, average='weighted')\n",
    "recall_nb = recall_score(y_test, y_pred_nb, average='weighted')\n",
    "f1_nb = f1_score(y_test, y_pred_nb, average='weighted')\n",
    "mcc_nb = matthews_corrcoef(y_test, y_pred_nb)\n",
    "\n",
    "# 4. Print results\n",
    "print(\"=== Gaussian Naive Bayes Classifier Results ===\")\n",
    "print(f\"Accuracy: {accuracy_nb:.4f}\")\n",
    "print(f\"AUC Score: {auc_nb:.4f}\")\n",
    "print(f\"Precision: {precision_nb:.4f}\")\n",
    "print(f\"Recall: {recall_nb:.4f}\")\n",
    "print(f\"F1 Score: {f1_nb:.4f}\")\n",
    "print(f\"MCC Score: {mcc_nb:.4f}\")\n",
    "\n",
    "# 5. Store results\n",
    "nb_results = {\n",
    "    'Model': 'Naive Bayes',\n",
    "    'Accuracy': accuracy_nb,\n",
    "    'AUC': auc_nb,\n",
    "    'Precision': precision_nb,\n",
    "    'Recall': recall_nb,\n",
    "    'F1': f1_nb,\n",
    "    'MCC': mcc_nb\n",
    "}\n",
    "\n",
    "# Add to results DataFrame\n",
    "results_df = pd.concat([results_df, pd.DataFrame([nb_results])], ignore_index=True)\n",
    "print(\"\\nNaive Bayes results added to comparison table.\")\n",
    "print(\"\\nCurrent Results Table:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Classifier Results ===\n",
      "Accuracy: 0.8800\n",
      "AUC Score: 0.9769\n",
      "Precision: 0.8796\n",
      "Recall: 0.8800\n",
      "F1 Score: 0.8797\n",
      "MCC Score: 0.8400\n",
      "\n",
      "Random Forest results added to comparison table.\n",
      "\n",
      "Current Results Table:\n",
      "                 Model  Accuracy       AUC  Precision  Recall        F1  \\\n",
      "0  Logistic Regression      0.67  0.896800   0.680872    0.67  0.674565   \n",
      "1        Decision Tree      0.83  0.886667   0.831883    0.83  0.830168   \n",
      "2  K-Nearest Neighbors      0.50  0.769750   0.521130    0.50  0.505355   \n",
      "3          Naive Bayes      0.81  0.950567   0.811326    0.81  0.810458   \n",
      "4        Random Forest      0.88  0.976929   0.879614    0.88  0.879734   \n",
      "\n",
      "        MCC  \n",
      "0  0.560537  \n",
      "1  0.773811  \n",
      "2  0.334993  \n",
      "3  0.746804  \n",
      "4  0.840049  \n"
     ]
    }
   ],
   "source": [
    "# Import Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. Initialize and train Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# 2. Make predictions\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "y_pred_proba_rf = rf_clf.predict_proba(X_test)\n",
    "\n",
    "# 3. Calculate metrics\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "auc_rf = roc_auc_score(y_test_bin, y_pred_proba_rf, multi_class='ovr', average='weighted')\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "mcc_rf = matthews_corrcoef(y_test, y_pred_rf)\n",
    "\n",
    "# 4. Print results\n",
    "print(\"=== Random Forest Classifier Results ===\")\n",
    "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "print(f\"AUC Score: {auc_rf:.4f}\")\n",
    "print(f\"Precision: {precision_rf:.4f}\")\n",
    "print(f\"Recall: {recall_rf:.4f}\")\n",
    "print(f\"F1 Score: {f1_rf:.4f}\")\n",
    "print(f\"MCC Score: {mcc_rf:.4f}\")\n",
    "\n",
    "# 5. Store results\n",
    "rf_results = {\n",
    "    'Model': 'Random Forest',\n",
    "    'Accuracy': accuracy_rf,\n",
    "    'AUC': auc_rf,\n",
    "    'Precision': precision_rf,\n",
    "    'Recall': recall_rf,\n",
    "    'F1': f1_rf,\n",
    "    'MCC': mcc_rf\n",
    "}\n",
    "\n",
    "# Add to results DataFrame\n",
    "results_df = pd.concat([results_df, pd.DataFrame([rf_results])], ignore_index=True)\n",
    "print(\"\\nRandom Forest results added to comparison table.\")\n",
    "print(\"\\nCurrent Results Table:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost Classifier Results ===\n",
      "Accuracy: 0.9225\n",
      "AUC Score: 0.9937\n",
      "Precision: 0.9226\n",
      "Recall: 0.9225\n",
      "F1 Score: 0.9225\n",
      "MCC Score: 0.8967\n",
      "\n",
      "XGBoost results added to comparison table.\n",
      "\n",
      "=== FINAL COMPARISON TABLE ===\n",
      "              Model  Accuracy      AUC  Precision  Recall       F1      MCC\n",
      "Logistic Regression    0.6700 0.896800   0.680872  0.6700 0.674565 0.560537\n",
      "      Decision Tree    0.8300 0.886667   0.831883  0.8300 0.830168 0.773811\n",
      "K-Nearest Neighbors    0.5000 0.769750   0.521130  0.5000 0.505355 0.334993\n",
      "        Naive Bayes    0.8100 0.950567   0.811326  0.8100 0.810458 0.746804\n",
      "      Random Forest    0.8800 0.976929   0.879614  0.8800 0.879734 0.840049\n",
      "            XGBoost    0.9225 0.993700   0.922631  0.9225 0.922482 0.896719\n"
     ]
    }
   ],
   "source": [
    "# Import XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# 1. Initialize and train XGBoost\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# 2. Make predictions\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_clf.predict_proba(X_test)\n",
    "\n",
    "# 3. Calculate metrics\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "auc_xgb = roc_auc_score(y_test_bin, y_pred_proba_xgb, multi_class='ovr', average='weighted')\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb, average='weighted')\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb, average='weighted')\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')\n",
    "mcc_xgb = matthews_corrcoef(y_test, y_pred_xgb)\n",
    "\n",
    "# 4. Print results\n",
    "print(\"=== XGBoost Classifier Results ===\")\n",
    "print(f\"Accuracy: {accuracy_xgb:.4f}\")\n",
    "print(f\"AUC Score: {auc_xgb:.4f}\")\n",
    "print(f\"Precision: {precision_xgb:.4f}\")\n",
    "print(f\"Recall: {recall_xgb:.4f}\")\n",
    "print(f\"F1 Score: {f1_xgb:.4f}\")\n",
    "print(f\"MCC Score: {mcc_xgb:.4f}\")\n",
    "\n",
    "# 5. Store results\n",
    "xgb_results = {\n",
    "    'Model': 'XGBoost',\n",
    "    'Accuracy': accuracy_xgb,\n",
    "    'AUC': auc_xgb,\n",
    "    'Precision': precision_xgb,\n",
    "    'Recall': recall_xgb,\n",
    "    'F1': f1_xgb,\n",
    "    'MCC': mcc_xgb\n",
    "}\n",
    "\n",
    "# Add to results DataFrame\n",
    "results_df = pd.concat([results_df, pd.DataFrame([xgb_results])], ignore_index=True)\n",
    "print(\"\\nXGBoost results added to comparison table.\")\n",
    "print(\"\\n=== FINAL COMPARISON TABLE ===\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models saved successfully in 'saved_models' directory!\n",
      "Models saved: ['logistic_regression.pkl', 'decision_tree.pkl', 'knn.pkl', 'naive_bayes.pkl', 'random_forest.pkl', 'xgboost.pkl', 'scaler.pkl']\n",
      "\n",
      "====================================================================================================\n",
      "FINAL MODEL COMPARISON TABLE\n",
      "====================================================================================================\n",
      "              Model Accuracy    AUC Precision Recall     F1    MCC\n",
      "Logistic Regression   0.6700 0.8968    0.6809 0.6700 0.6746 0.5605\n",
      "      Decision Tree   0.8300 0.8867    0.8319 0.8300 0.8302 0.7738\n",
      "K-Nearest Neighbors   0.5000 0.7697    0.5211 0.5000 0.5054 0.3350\n",
      "        Naive Bayes   0.8100 0.9506    0.8113 0.8100 0.8105 0.7468\n",
      "      Random Forest   0.8800 0.9769    0.8796 0.8800 0.8797 0.8400\n",
      "            XGBoost   0.9225 0.9937    0.9226 0.9225 0.9225 0.8967\n",
      "\n",
      "Results saved to 'model_comparison_results.csv'\n",
      "\n",
      "====================================================================================================\n",
      "BEST PERFORMING MODELS FOR EACH METRIC\n",
      "====================================================================================================\n",
      "Accuracy: XGBoost (0.9225)\n",
      "AUC: XGBoost (0.9937)\n",
      "Precision: XGBoost (0.9226)\n",
      "Recall: XGBoost (0.9225)\n",
      "F1: XGBoost (0.9225)\n",
      "MCC: XGBoost (0.8967)\n"
     ]
    }
   ],
   "source": [
    "# Import joblib for saving models\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# 1. Create a directory to save models\n",
    "model_dir = 'saved_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# 2. Save all trained models\n",
    "joblib.dump(log_reg, f'{model_dir}/logistic_regression.pkl')\n",
    "joblib.dump(dt_clf, f'{model_dir}/decision_tree.pkl')\n",
    "joblib.dump(knn_clf, f'{model_dir}/knn.pkl')\n",
    "joblib.dump(nb_clf, f'{model_dir}/naive_bayes.pkl')\n",
    "joblib.dump(rf_clf, f'{model_dir}/random_forest.pkl')\n",
    "joblib.dump(xgb_clf, f'{model_dir}/xgboost.pkl')\n",
    "joblib.dump(scaler, f'{model_dir}/scaler.pkl')  # Save the scaler for KNN\n",
    "\n",
    "print(\"All models saved successfully in 'saved_models' directory!\")\n",
    "print(f\"Models saved: {os.listdir(model_dir)}\")\n",
    "\n",
    "# 3. Create a well-formatted comparison table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Format the results for better display\n",
    "formatted_results = results_df.copy()\n",
    "\n",
    "# Round all metric values to 4 decimal places\n",
    "metric_columns = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']\n",
    "for col in metric_columns:\n",
    "    formatted_results[col] = formatted_results[col].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# Display the table\n",
    "print(formatted_results.to_string(index=False))\n",
    "\n",
    "# 4. Save results to CSV for reference\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'model_comparison_results.csv'\")\n",
    "\n",
    "# 5. Identify best model for each metric\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BEST PERFORMING MODELS FOR EACH METRIC\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for metric in metric_columns:\n",
    "    if metric in ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']:\n",
    "        # Find the model with highest value for this metric\n",
    "        best_idx = results_df[metric].idxmax()\n",
    "        best_model = results_df.loc[best_idx, 'Model']\n",
    "        best_value = results_df.loc[best_idx, metric]\n",
    "        print(f\"{metric}: {best_model} ({best_value:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 20:31:41.017 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.018 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.245 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/cloud/.local/lib/python3.9/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2026-02-06 20:31:41.247 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.248 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.250 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.250 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.251 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.253 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.255 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.256 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.257 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.260 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.262 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.263 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.264 Session state does not function when running a script without `streamlit run`\n",
      "2026-02-06 20:31:41.266 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.267 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.270 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.280 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.283 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.285 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.286 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.288 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.290 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.291 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.292 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.295 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.297 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.299 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.300 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.302 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.305 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.307 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.309 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.311 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2026-02-06 20:31:41.311 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=1, _parent=DeltaGenerator())"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Mobile Price Classification\",\n",
    "    page_icon=\"üì±\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Title\n",
    "st.title(\"üì± Mobile Price Classification App\")\n",
    "st.markdown(\"\"\"\n",
    "This app predicts mobile phone price range (0-3) based on various specifications.\n",
    "Upload your test data (CSV) or use the sample data to see predictions.\n",
    "\"\"\")\n",
    "\n",
    "# Sidebar for navigation\n",
    "st.sidebar.title(\"Navigation\")\n",
    "options = st.sidebar.radio(\"Select a page:\", \n",
    "                          [\"Home\", \"Upload & Predict\", \"Model Comparison\", \"About\"])\n",
    "\n",
    "# Load saved models\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    models = {\n",
    "        \"Logistic Regression\": joblib.load('saved_models/logistic_regression.pkl'),\n",
    "        \"Decision Tree\": joblib.load('saved_models/decision_tree.pkl'),\n",
    "        \"K-Nearest Neighbors\": joblib.load('saved_models/knn.pkl'),\n",
    "        \"Naive Bayes\": joblib.load('saved_models/naive_bayes.pkl'),\n",
    "        \"Random Forest\": joblib.load('saved_models/random_forest.pkl'),\n",
    "        \"XGBoost\": joblib.load('saved_models/xgboost.pkl')\n",
    "    }\n",
    "    scaler = joblib.load('saved_models/scaler.pkl')\n",
    "    return models, scaler\n",
    "\n",
    "# Home Page\n",
    "if options == \"Home\":\n",
    "    st.header(\"Welcome to Mobile Price Classifier\")\n",
    "    st.markdown(\"\"\"\n",
    "    ### Dataset Description\n",
    "    This dataset contains information about 2000 mobile phones with the following features:\n",
    "    \n",
    "    **Features (20):**\n",
    "    1. **battery_power** - Total energy a battery can store (mAh)\n",
    "    2. **blue** - Has Bluetooth or not (0/1)\n",
    "    3. **clock_speed** - Speed of microprocessor (GHz)\n",
    "    4. **dual_sim** - Has dual sim support (0/1)\n",
    "    5. **fc** - Front camera megapixels\n",
    "    6. **four_g** - Has 4G or not (0/1)\n",
    "    7. **int_memory** - Internal memory (GB)\n",
    "    8. **m_dep** - Mobile depth (cm)\n",
    "    9. **mobile_wt** - Weight of mobile phone\n",
    "    10. **n_cores** - Number of cores of processor\n",
    "    11. **pc** - Primary camera megapixels\n",
    "    12. **px_height** - Pixel resolution height\n",
    "    13. **px_width** - Pixel resolution width\n",
    "    14. **ram** - Random Access Memory (MB)\n",
    "    15. **sc_h** - Screen height (cm)\n",
    "    16. **sc_w** - Screen width (cm)\n",
    "    17. **talk_time** - Longest battery life on single charge (hours)\n",
    "    18. **three_g** - Has 3G or not (0/1)\n",
    "    19. **touch_screen** - Has touch screen or not (0/1)\n",
    "    20. **wifi** - Has wifi or not (0/1)\n",
    "    \n",
    "    **Target:**\n",
    "    - **price_range** - Price class (0: low cost, 1: medium cost, 2: high cost, 3: very high cost)\n",
    "    \n",
    "    ### Models Implemented\n",
    "    1. Logistic Regression\n",
    "    2. Decision Tree Classifier\n",
    "    3. K-Nearest Neighbor Classifier\n",
    "    4. Naive Bayes Classifier\n",
    "    5. Random Forest (Ensemble)\n",
    "    6. XGBoost (Ensemble)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Show sample data\n",
    "    if st.checkbox(\"Show sample data\"):\n",
    "        sample_data = pd.read_csv('mobile_price.csv').head(10)\n",
    "        st.dataframe(sample_data)\n",
    "\n",
    "# Upload & Predict Page\n",
    "elif options == \"Upload & Predict\":\n",
    "    st.header(\"üì§ Upload Test Data and Predict\")\n",
    "    \n",
    "    # Option 1: Upload CSV\n",
    "    uploaded_file = st.file_uploader(\"Upload your test CSV file (without price_range column)\", type=['csv'])\n",
    "    \n",
    "    # Option 2: Use sample test data\n",
    "    use_sample = st.checkbox(\"Use sample test data\")\n",
    "    \n",
    "    if uploaded_file is not None or use_sample:\n",
    "        try:\n",
    "            # Load data\n",
    "            if uploaded_file is not None:\n",
    "                test_df = pd.read_csv(uploaded_file)\n",
    "            else:\n",
    "                # Use last 100 rows as sample test data\n",
    "                full_data = pd.read_csv('mobile_price.csv')\n",
    "                test_df = full_data.drop('price_range', axis=1).tail(100)\n",
    "            \n",
    "            st.success(\"Data loaded successfully!\")\n",
    "            st.write(f\"**Shape:** {test_df.shape}\")\n",
    "            st.write(\"**First 5 rows:**\")\n",
    "            st.dataframe(test_df.head())\n",
    "            \n",
    "            # Check if all required columns are present\n",
    "            required_columns = ['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n",
    "                               'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',\n",
    "                               'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',\n",
    "                               'touch_screen', 'wifi']\n",
    "            \n",
    "            missing_cols = [col for col in required_columns if col not in test_df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                st.error(f\"Missing columns: {missing_cols}\")\n",
    "            else:\n",
    "                # Model selection\n",
    "                st.subheader(\"Select Model for Prediction\")\n",
    "                model_names = [\"Logistic Regression\", \"Decision Tree\", \"K-Nearest Neighbors\", \n",
    "                              \"Naive Bayes\", \"Random Forest\", \"XGBoost\"]\n",
    "                selected_model = st.selectbox(\"Choose a model:\", model_names)\n",
    "                \n",
    "                if st.button(\"Predict\"):\n",
    "                    # Load models\n",
    "                    models, scaler = load_models()\n",
    "                    model = models[selected_model]\n",
    "                    \n",
    "                    # Prepare data (scale if KNN)\n",
    "                    if selected_model == \"K-Nearest Neighbors\":\n",
    "                        X_test_scaled = scaler.transform(test_df)\n",
    "                        predictions = model.predict(X_test_scaled)\n",
    "                    else:\n",
    "                        predictions = model.predict(test_df)\n",
    "                    \n",
    "                    # Add predictions to dataframe\n",
    "                    results_df = test_df.copy()\n",
    "                    results_df['Predicted_Price_Range'] = predictions\n",
    "                    \n",
    "                    # Map price range to labels\n",
    "                    price_labels = {0: \"Low Cost\", 1: \"Medium Cost\", 2: \"High Cost\", 3: \"Very High Cost\"}\n",
    "                    results_df['Predicted_Label'] = results_df['Predicted_Price_Range'].map(price_labels)\n",
    "                    \n",
    "                    # Display results\n",
    "                    st.subheader(\"üìä Prediction Results\")\n",
    "                    st.dataframe(results_df[['Predicted_Price_Range', 'Predicted_Label']].head(20))\n",
    "                    \n",
    "                    # Show distribution\n",
    "                    st.subheader(\"üìà Prediction Distribution\")\n",
    "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                    results_df['Predicted_Label'].value_counts().plot(kind='bar', ax=ax, color='skyblue')\n",
    "                    ax.set_title('Distribution of Predicted Price Ranges')\n",
    "                    ax.set_xlabel('Price Range')\n",
    "                    ax.set_ylabel('Count')\n",
    "                    plt.xticks(rotation=45)\n",
    "                    st.pyplot(fig)\n",
    "                    \n",
    "                    # Download results\n",
    "                    csv = results_df.to_csv(index=False)\n",
    "                    st.download_button(\n",
    "                        label=\"Download Predictions as CSV\",\n",
    "                        data=csv,\n",
    "                        file_name=\"mobile_price_predictions.csv\",\n",
    "                        mime=\"text/csv\"\n",
    "                    )\n",
    "                    \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error: {str(e)}\")\n",
    "\n",
    "# Model Comparison Page\n",
    "elif options == \"Model Comparison\":\n",
    "    st.header(\"üìä Model Performance Comparison\")\n",
    "    \n",
    "    # Load pre-calculated results\n",
    "    try:\n",
    "        results_df = pd.read_csv('model_comparison_results.csv')\n",
    "        \n",
    "        # Display metrics table\n",
    "        st.subheader(\"Evaluation Metrics Table\")\n",
    "        st.dataframe(results_df.style.format({\n",
    "            'Accuracy': '{:.4f}',\n",
    "            'AUC': '{:.4f}',\n",
    "            'Precision': '{:.4f}',\n",
    "            'Recall': '{:.4f}',\n",
    "            'F1': '{:.4f}',\n",
    "            'MCC': '{:.4f}'\n",
    "        }))\n",
    "        \n",
    "        # Create comparison chart\n",
    "        st.subheader(\"Model Performance Visualization\")\n",
    "        \n",
    "        # Let user select metric to visualize\n",
    "        metric_to_plot = st.selectbox(\"Select metric to visualize:\", \n",
    "                                     ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC'])\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        bars = ax.bar(results_df['Model'], results_df[metric_to_plot], color='lightcoral')\n",
    "        ax.set_title(f'{metric_to_plot} Comparison Across Models')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_ylabel(metric_to_plot)\n",
    "        ax.set_ylim([0, 1])\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        st.pyplot(fig)\n",
    "        \n",
    "        # Show observations\n",
    "        st.subheader(\"üîç Model Performance Observations\")\n",
    "        st.markdown(\"\"\"\n",
    "        **Logistic Regression**: Simple linear model, decent performance for baseline.\n",
    "        \n",
    "        **Decision Tree**: May overfit but provides good interpretability.\n",
    "        \n",
    "        **K-Nearest Neighbors**: Distance-based, requires feature scaling.\n",
    "        \n",
    "        **Naive Bayes**: Fast but assumes feature independence.\n",
    "        \n",
    "        **Random Forest**: Ensemble method, reduces overfitting, generally robust.\n",
    "        \n",
    "        **XGBoost**: Advanced ensemble, often best for tabular data.\n",
    "        \"\"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading results: {str(e)}\")\n",
    "        st.info(\"Please run the model training notebook first to generate comparison results.\")\n",
    "\n",
    "# About Page\n",
    "elif options == \"About\":\n",
    "    st.header(\"‚ÑπÔ∏è About This Project\")\n",
    "    st.markdown(\"\"\"\n",
    "    ### Machine Learning Assignment 2\n",
    "    \n",
    "    **Objective**: Implement and compare 6 classification models for mobile price prediction.\n",
    "    \n",
    "    **Models Implemented**:\n",
    "    1. Logistic Regression\n",
    "    2. Decision Tree Classifier\n",
    "    3. K-Nearest Neighbor Classifier\n",
    "    4. Naive Bayes Classifier\n",
    "    5. Random Forest (Ensemble)\n",
    "    6. XGBoost (Ensemble)\n",
    "    \n",
    "    **Evaluation Metrics**:\n",
    "    - Accuracy\n",
    "    - AUC Score\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1 Score\n",
    "    - Matthews Correlation Coefficient (MCC)\n",
    "    \n",
    "    **Dataset**: Mobile Price Classification Dataset (2000 samples, 20 features)\n",
    "    \n",
    "    **Deployment**: Streamlit Web Application\n",
    "    \n",
    "    ### How to Use\n",
    "    1. Go to **Upload & Predict** page to test the models\n",
    "    2. Upload a CSV file with mobile features (without price_range)\n",
    "    3. Select a model from dropdown\n",
    "    4. Click Predict to see results\n",
    "    5. Check **Model Comparison** page to see performance metrics\n",
    "    \n",
    "    ### Technical Details\n",
    "    - Built with Python, Scikit-learn, XGBoost\n",
    "    - Web interface: Streamlit\n",
    "    - Deployment: Streamlit Community Cloud\n",
    "    \"\"\")\n",
    "    \n",
    "    st.info(\"**Note**: For assignment submission, this app is deployed on Streamlit Community Cloud with all required features.\")\n",
    "\n",
    "# Footer\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.info(\n",
    "    \"\"\"\n",
    "    **Assignment 2 - Machine Learning**  \n",
    "    M.Tech (AIML/DSE)  \n",
    "    BITS Pilani WILP  \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ACTUAL METRICS FOR README.md ===\n",
      "\n",
      "Copy and paste this table into your README.md:\n",
      "\n",
      "| ML Model Name | Accuracy | AUC | Precision | Recall | F1 | MCC |\n",
      "|---------------|----------|-----|-----------|--------|----|-----|\n",
      "| Logistic Regression | 0.6700 | 0.8968 | 0.6809 | 0.6700 | 0.6746 | 0.5605 |\n",
      "| Decision Tree | 0.8300 | 0.8867 | 0.8319 | 0.8300 | 0.8302 | 0.7738 |\n",
      "| K-Nearest Neighbors | 0.5000 | 0.7697 | 0.5211 | 0.5000 | 0.5054 | 0.3350 |\n",
      "| Naive Bayes | 0.8100 | 0.9506 | 0.8113 | 0.8100 | 0.8105 | 0.7468 |\n",
      "| Random Forest | 0.8800 | 0.9769 | 0.8796 | 0.8800 | 0.8797 | 0.8400 |\n",
      "| XGBoost | 0.9225 | 0.9937 | 0.9226 | 0.9225 | 0.9225 | 0.8967 |\n",
      "\n",
      "=== OBSERVATIONS TABLE ===\n",
      "\n",
      "Copy this format for your observations:\n",
      "\n",
      "| ML Model Name | Observation about model performance |\n",
      "|---------------|--------------------------------------|\n",
      "| Logistic Regression | [Your observation based on your results] |\n",
      "| Decision Tree | [Your observation based on your results] |\n",
      "| K-Nearest Neighbors | [Your observation based on your results] |\n",
      "| Naive Bayes | [Your observation based on your results] |\n",
      "| Random Forest (Ensemble) | [Your observation based on your results] |\n",
      "| XGBoost (Ensemble) | [Your observation based on your results] |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get your actual metrics in the correct format\n",
    "print(\"=== ACTUAL METRICS FOR README.md ===\")\n",
    "print(\"\\nCopy and paste this table into your README.md:\\n\")\n",
    "\n",
    "# Print the markdown table header\n",
    "print(\"| ML Model Name | Accuracy | AUC | Precision | Recall | F1 | MCC |\")\n",
    "print(\"|---------------|----------|-----|-----------|--------|----|-----|\")\n",
    "\n",
    "# Print each row with your actual values\n",
    "for index, row in results_df.iterrows():\n",
    "    print(f\"| {row['Model']} | {row['Accuracy']:.4f} | {row['AUC']:.4f} | {row['Precision']:.4f} | {row['Recall']:.4f} | {row['F1']:.4f} | {row['MCC']:.4f} |\")\n",
    "\n",
    "print(\"\\n=== OBSERVATIONS TABLE ===\")\n",
    "print(\"\\nCopy this format for your observations:\")\n",
    "print(\"\"\"\n",
    "| ML Model Name | Observation about model performance |\n",
    "|---------------|--------------------------------------|\n",
    "| Logistic Regression | [Your observation based on your results] |\n",
    "| Decision Tree | [Your observation based on your results] |\n",
    "| K-Nearest Neighbors | [Your observation based on your results] |\n",
    "| Naive Bayes | [Your observation based on your results] |\n",
    "| Random Forest (Ensemble) | [Your observation based on your results] |\n",
    "| XGBoost (Ensemble) | [Your observation based on your results] |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MISSING METRICS ===\n",
      "Run this code to get Logistic Regression and Decision Tree metrics:\n",
      "\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.6700\n",
      "  AUC: 0.8968\n",
      "  Precision: 0.6809\n",
      "  Recall: 0.6700\n",
      "  F1: 0.6746\n",
      "  MCC: 0.5605\n",
      "\n",
      "Decision Tree:\n",
      "  Accuracy: 0.8300\n",
      "  AUC: 0.8867\n",
      "  Precision: 0.8319\n",
      "  Recall: 0.8300\n",
      "  F1: 0.8302\n",
      "  MCC: 0.7738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get missing metrics\n",
    "print(\"=== MISSING METRICS ===\")\n",
    "print(\"Run this code to get Logistic Regression and Decision Tree metrics:\\n\")\n",
    "\n",
    "# Filter for Logistic Regression and Decision Tree\n",
    "missing_models = results_df[results_df['Model'].isin(['Logistic Regression', 'Decision Tree'])]\n",
    "\n",
    "for index, row in missing_models.iterrows():\n",
    "    print(f\"{row['Model']}:\")\n",
    "    print(f\"  Accuracy: {row['Accuracy']:.4f}\")\n",
    "    print(f\"  AUC: {row['AUC']:.4f}\")\n",
    "    print(f\"  Precision: {row['Precision']:.4f}\")\n",
    "    print(f\"  Recall: {row['Recall']:.4f}\")\n",
    "    print(f\"  F1: {row['F1']:.4f}\")\n",
    "    print(f\"  MCC: {row['MCC']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3737097518.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    streamlit run app.py\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
